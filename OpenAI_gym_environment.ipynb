{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI_gym_environment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiago1Ribeiro/OpenAI_gym_environment/blob/master/OpenAI_gym_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uwk8zVBMxI-r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Testing Gym (OpenAI): https://gym.openai.com/docs/"
      ]
    },
    {
      "metadata": {
        "id": "le_9Sq0kXOJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Other referencies:\n",
        "[Understanding OpenAI Gym](https://medium.com/@ashish_fagna/understanding-openai-gym-25c79c06eccb)\n",
        "\n",
        "[CartPole-v1 environment](https://gym.openai.com/envs/CartPole-v1)\n",
        "\n",
        "[Introduction: Reinforcement Learning with OpenAI Gym - **Q-Learning** ](https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2)\n",
        "\n",
        "[Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym](https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html)"
      ]
    },
    {
      "metadata": {
        "id": "7TPxcPdOKk0k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Install dependencies"
      ]
    },
    {
      "metadata": {
        "id": "Zfb8Te7UCMPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install xvfbwrapper\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "is81wQZVKfTL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "7OuevKj2w760",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pyglet\n",
        "import gym\n",
        "import PIL.Image\n",
        "import io\n",
        "import pandas\n",
        "\n",
        "import matplotlib\n",
        "import setuptools\n",
        "\n",
        "import tensorflow\n",
        "#from tensorflow.contrib.data.python.util import nest\n",
        "#import click\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZfXRjStlIz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Q-learning algorithm (RL) class\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637)"
      ]
    },
    {
      "metadata": {
        "id": "RzDKvZ4VlIBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class QLearn:\n",
        "    \n",
        "    def __init__(self, actions, epsilon, alpha, gamma):\n",
        "        self.q = {}             # initialize with noise ?\n",
        "\n",
        "        self.epsilon = epsilon  # exploration constant\n",
        "        self.alpha = alpha      # learning rate\n",
        "        self.gamma = gamma      # discount factor\n",
        "        self.actions = actions\n",
        "        \n",
        "    def getQ(self, state, action):\n",
        "        # dictionary.get(keyname, value); A value to return if the specified key do not exist. Default value None\n",
        "        return self.q.get((state, action), 0.0)\n",
        "      \n",
        "    def learnQ(self, state, action, reward, value):\n",
        "        '''\n",
        "        Q-learning:\n",
        "            Qnew(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))            \n",
        "        '''\n",
        "        Qold = self.q.get((state, action), None)\n",
        "        if Qold is None:\n",
        "            self.q[(state, action)] = reward\n",
        "        else:\n",
        "            self.q[(state, action)] = Qold + self.alpha * (value - Qold)\n",
        "            \n",
        "            \n",
        "    def chooseAction(self, state, return_q=False):\n",
        "        \n",
        "        q = [self.getQ(state, a) for a in self.actions]\n",
        "        maxQ = max(q)\n",
        "        \n",
        "        # magic numbers?\n",
        "        if np.random.random() < self.epsilon:\n",
        "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
        "            # add random values to all the actions, recalculate maxQ\n",
        "            q = [q[i] + np.random.random() * mag - .5 * mag for i in range(len(self.actions))] \n",
        "            maxQ = max(q)\n",
        "\n",
        "        count = q.count(maxQ)\n",
        "        # In case there're several state-action max values \n",
        "        # we select a random one among them\n",
        "        if count > 1:\n",
        "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
        "            i = np.random.choice(best)\n",
        "        else:\n",
        "            i = q.index(maxQ)\n",
        "\n",
        "        action = self.actions[i]        \n",
        "        if return_q: # if they want it, give it!\n",
        "            return action, q\n",
        "        return action\n",
        "\n",
        "    def learn(self, state1, action1, reward, state2):\n",
        "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
        "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
        "        \n",
        "    \n",
        "    ########################\n",
        "    def build_state(features):    \n",
        "        return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
        "\n",
        "    def to_bin(value, bins):\n",
        "        return np.digitize(x=[value], bins=bins)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GH6jBdxlJQES",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Class to Discretize Box space "
      ]
    },
    {
      "metadata": {
        "id": "wdRtbYY3JQUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DiscretizedObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"This wrapper converts a Box observation into a single integer.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_bins=10, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box) # verifies condition; if false, gives error \n",
        "\n",
        "        low = self.observation_space.low if low is None else low\n",
        "        high = self.observation_space.high if high is None else high\n",
        "\n",
        "        self.n_bins = n_bins\n",
        "        #print('low: '+ 'teste'.join(low))\n",
        "        #print('high: '+ 'teste_'.join(low))\n",
        "        # numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0); Returns num evenly spaced samples, calculated over the interval [start, stop]\n",
        "        self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in\n",
        "                         zip(low.flatten(), high.flatten())]\n",
        "                         #zip(low.flatten(), high.flatten())]\n",
        "        \n",
        "        self.observation_space = gym.spaces.Discrete(n_bins ** low.flatten().shape[0])\n",
        "\n",
        "    def _convert_to_one_number(self, digits):\n",
        "        return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)])\n",
        "\n",
        "    def observation(self, observation):\n",
        "        digits = [np.digitize([x], bins)[0]\n",
        "                  for x, bins in zip(observation.flatten(), self.val_bins)]\n",
        "        return self._convert_to_one_number(digits)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEekRfTbphIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Loads Environment and Maps State (Observations) and Action table  \n",
        "\n",
        "Q table contains state-action pairs mapping to reward. So, we will construct an array which maps different state and actions to reward values during run of algorithm. Its dimension will clearly |states|x|actions|."
      ]
    },
    {
      "metadata": {
        "id": "6wgNPTOmT-ih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Spaces\n",
        "Every environment comes with an action_space and an observation_space. These attributes are of type Space, and they describe the format of valid actions and observations.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> ![alt text](https://cdn-images-1.medium.com/max/800/1*7Ae4mf9gVvpuMgenwtf8wA.png)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " **Discrete** space allows a fixed range of non-negative numbers, so in this case valid **actions** are either 0 or 1.\n",
        "\n",
        "The **Box** space represents an n-dimensional box, so valid **observations** will be an array of 4 numbers\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "f3JJUQO8TyWP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#env = gym.make('CartPole-v1')\n",
        "#print(env.action_space); print(env.observation_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j8U-trmDLgtb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Virtual Display to [Colab](https://colab.research.google.com)"
      ]
    },
    {
      "metadata": {
        "id": "UhAQC6RfHyc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Virtual display (Colab.google)\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display\n",
        "from xvfbwrapper import Xvfb\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "vdisplay = Xvfb(width=1280, height=740)\n",
        "vdisplay.start()\n",
        "\n",
        "#env = gym.make('CartPole-v0')\n",
        "#env.reset()\n",
        "\n",
        "def showarray(a, fmt='png'):\n",
        "    a = np.uint8(a)\n",
        "    f = io.BytesIO()\n",
        "    ima = PIL.Image.fromarray(a).save(f, fmt)\n",
        "    return f.getvalue()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "htxH7vv2v64B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "IHxww43iNuiL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    env = gym.make('CartPole-v1')\n",
        "    env.reset()\n",
        "    #imagehandle = display.display(display.Image(data=showarray(env.render(mode='rgb_array')), width=450), display_id='gymscr')\n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "     Observation: \n",
        "        Type: Box(4)\n",
        "        Num\tObservation                 Min         Max\n",
        "        0\tCart Position             -4.8            4.8\n",
        "        1\tCart Velocity             -Inf            Inf\n",
        "        2\tPole Angle                 -24°           24°\n",
        "        3\tPole Velocity At Tip      -Inf            Inf\n",
        "        \n",
        "    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num\tAction\n",
        "        0\tPush cart to the left\n",
        "        1\tPush cart to the right\n",
        "    \"\"\"\n",
        "   \n",
        "    env = DiscretizedObservationWrapper(\n",
        "        env, \n",
        "        n_bins=8, \n",
        "        low=np.array([-2.4, -2.0, -0.42, -3.5]), \n",
        "        high=np.array([2.4, 2.0, 0.42, 3.5])\n",
        "    )\n",
        "    \n",
        "    goal_average_steps = 195\n",
        "    max_number_of_steps = 10000\n",
        "    n_bins = 1\n",
        "    last_time_steps = np.ndarray(0)\n",
        "    # The Q-learn algorithm\n",
        "    qlearn = QLearn(actions=range(env.action_space.n),\n",
        "                    alpha=0.5, gamma=0.90, epsilon=0.1)\n",
        "    \n",
        "    for i_episode in range(20000):\n",
        "        state = env.reset()\n",
        "    \n",
        "        qlearn.epsilon = qlearn.epsilon * 0.999 # added epsilon decay\n",
        "        cumulated_reward = 0\n",
        "        for t in range(max_number_of_steps):\t    \t\n",
        "            \n",
        "            \n",
        "            # Pick an action based on the current state\n",
        "            action = qlearn.chooseAction(state)\n",
        "            #print(action)\n",
        "            \n",
        "            # Execute the action and get feedback\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            \n",
        "            nextState = obs\n",
        "            \n",
        "            # TODO remove\n",
        "            #if reward != -1:\n",
        "                #print (reward)\n",
        "\n",
        "            qlearn.learn(state, action, reward, nextState)\n",
        "            state = nextState\n",
        "            cumulated_reward += reward\n",
        "\n",
        "            if done:\n",
        "                last_time_steps = np.append(last_time_steps, [int(t + 1)])\n",
        "                break\n",
        "            \n",
        "            #time.sleep(0.01)    # Pauses program (secs)\n",
        "            #display.update_display(display.Image(data=showarray(env.render(mode='rgb_array')), width=450), display_id='gymscr')\n",
        "        \n",
        "        #vdisplay.stop()\n",
        "        #print(\"Episode {:d} reward score: {:0.2f}\".format(i_episode, cumulated_reward))\n",
        "\n",
        "    \n",
        "    \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEj5ctNnhuKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Episode {:d} reward score: {:0.2f}\".format(i_episode, cumulated_reward))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L35yJLrewDKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    env = gym.make('CartPole-v1')\n",
        "    # env.monitor.start('/tmp/cartpole-experiment-1', force=True)\n",
        "        # video_callable=lambda count: count % 10 == 0)\n",
        "    \"\"\"\n",
        "     Observation: \n",
        "        Type: Box(4)\n",
        "        Num\tObservation                 Min         Max\n",
        "        0\tCart Position             -4.8            4.8\n",
        "        1\tCart Velocity             -Inf            Inf\n",
        "        2\tPole Angle                 -24°           24°\n",
        "        3\tPole Velocity At Tip      -Inf            Inf\n",
        "        \n",
        "    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num\tAction\n",
        "        0\tPush cart to the left\n",
        "        1\tPush cart to the right\n",
        "    \"\"\"\n",
        "\n",
        "    env = DiscretizedObservationWrapper(\n",
        "        env, \n",
        "        n_bins=8, \n",
        "        low=[-2.4, -2.0, -0.42, -3.5], \n",
        "        high=[2.4, 2.0, 0.42, 3.5]\n",
        "    )\n",
        "    print(env.observation_space)\n",
        "    \n",
        "    goal_average_steps = 195\n",
        "    max_number_of_steps = 1000\n",
        "    #last_time_steps = numpy.ndarray(0)\n",
        "    n_bins = 1\n",
        "\n",
        "    \n",
        "    last_time_steps = numpy.ndarray(0)\n",
        "\n",
        "    #env.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
        "    \n",
        "    number_of_features = env.observation_space.shape[0]\n",
        "    # Number of states is huge so in order to simplify the situation\n",
        "    # we discretize the space to: 10 ** number_of_features\n",
        "    feature1_bins = pandas.cut([-1.2, 0.6], bins=n_bins, retbins=True)[1][1:-1]\n",
        "    feature2_bins = pandas.cut([-0.07, 0.07], bins=n_bins, retbins=True)[1][1:-1]\n",
        "\n",
        "    # The Q-learn algorithm\n",
        "    qlearn = QLearn(actions=range(env.action_space.n),\n",
        "                    alpha=0.5, gamma=0.90, epsilon=0.1)\n",
        "\n",
        "    for i_episode in xrange(200):\n",
        "        observation = env.reset()\n",
        "\n",
        "        feature1, feature2 = observation            \n",
        "        state = build_state([to_bin(feature1, feature1_bins),\n",
        "                         to_bin(feature2, feature2_bins)])\n",
        "\n",
        "        qlearn.epsilon = qlearn.epsilon * 0.999 # added epsilon decay\n",
        "        cumulated_reward = 0\n",
        "\n",
        "        for t in xrange(max_number_of_steps):\t    \t\n",
        "            \n",
        "            \n",
        "            # Pick an action based on the current state\n",
        "            action = qlearn.chooseAction(state)\n",
        "            print(action)\n",
        "            # Execute the action and get feedback\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            # Digitize the observation to get a state\n",
        "            feature1, feature2 = observation            \n",
        "            nextState = build_state([to_bin(feature1, feature1_bins),\n",
        "                             to_bin(feature2, feature2_bins)])\n",
        "\n",
        "            # TODO remove\n",
        "            if reward != -1:\n",
        "                print reward\n",
        "\n",
        "            qlearn.learn(state, action, reward, nextState)\n",
        "            state = nextState\n",
        "            cumulated_reward += reward\n",
        "\n",
        "            if done:\n",
        "                last_time_steps = numpy.append(last_time_steps, [int(t + 1)])\n",
        "                break\n",
        "            \n",
        "            time.sleep(0.01)    # Pauses program (secs)\n",
        "            display.update_display(display.Image(data=showarray(env.render(mode='rgb_array')), width=450), display_id='gymscr')\n",
        "        \n",
        "        vdisplay.stop()\n",
        "        print(\"Episode {:d} reward score: {:0.2f}\".format(i_episode, cumulated_reward))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ldzJ2Rwjv5Fm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i_episode in range(20):\n",
        "    observation = env.reset()\n",
        "    rAll = 0\n",
        "    done = False\n",
        "    j = 0\n",
        "    \n",
        "    for t in range(1000):   # Runs an instance of ‘CartPole-v0’ env for 1000 timesteps\n",
        "        time.sleep(0.01)    # Pauses program (secs)\n",
        "        \n",
        "        # Choose action from Q table\n",
        "        action = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "        #Get new state & reward from environment\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[s,action] = Q[s,action] + eta*(r + gma*np.max(Q[s1,:]) - Q[s,action])\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "        display.update_display(display.Image(data=showarray(env.render(mode='rgb_array')), width=450), display_id='gymscr')\n",
        "        rev_list.append(rAll)\n",
        "vdisplay.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}